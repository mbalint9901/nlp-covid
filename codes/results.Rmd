---
title: "NLP-COVID - Results"
author: "Marcell P. Granát & Bálint Mazzag"
date: '2021 04 23 '
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center",
                      warning = FALSE, message = FALSE, error = TRUE,
                      fig.path = paste0(getwd(), "/figures"))
```

# Setup

```{r packages}
# Set up --------------------------------------------------------------------------------

## Packages ============================================================================= 

library(tidyverse)
library(knitr)
library(broom)
library(geofacet)
library(tidytext)
library(tm)
library(wordcloud)
library(lubridate)
library(knitr)

ggthemr::ggthemr("greyscale")

```


```{r theme, include=FALSE}
## Gg theme =============================================================================

update_geom_defaults("point", list(fill = "cyan4", 
                                   shape = 21, 
                                   color = "black", 
                                   size = 1.4))
update_geom_defaults("line", 
                     list(color = viridis::viridis_pal()(20)[6], size = 1.4))

update_geom_defaults("smooth", list(color = "cyan4", size = 1.4))

update_geom_defaults("density", 
                     list(color = "midnightblue", fill =  "midnightblue",
                          alpha = .3, size = 1.4))

extrafont::loadfonts(device="win")

theme_set(theme_grey() + theme(
  legend.direction = "vertical",
  plot.caption = element_text(family = "serif")
))

#Sys.setlocale(locale ='Hungarian')

```

# Data

```{r}
# Data ----------------------------------------------------------------------------------

# DFs from the cleaning.R file ==========================================================

dat <- all_news()


# Topic models ==========================================================================

# Topic models calculated in -> topic_models.R <-
# moved this calculations to different files due to the large computation time
# Posterior estimation of each article with topic models is also computation heavy
# >> find the estimation in -> dat_topics.R <-

dat_topics <- read_rds("data/dat_topics.rds")

# Sentiment scores  ==========================================================================

# Sentiment scores are calculated by a dictionary based on topics -> sentiment_scores.R <-
# moved this calculations to different files due to the large computation time

load("data/sentiment_scores_results.RData")

# Word frequency by month ====================================================================

dat_words_monthly <- read_rds("data/dat_words_monthly.RDS")

# Additional datasets available online from Covid cases to economic sentiment

source("codes/data_setup.R") 

# This RData contains the articles after the main cleaning process
# To ensure full reproducibility see the attached files at the corresponding
# GitHub Repo: -> https://github.com/MarcellGranat/CoronaSentiment <-

### COVID-dictionary ####################################################################

# own edited sentiment dictionary calibrated to COVID articles
# Taken into account the Bing original dictionary and altering the 2000 most frequent
# sentiment words in the dictionary

modified_sentiment_dictionary <- 
  read_delim("data/modified_sentiment_dictionary.csv",
             delim = ";", escape_double = FALSE)

modified_bing <- 
  modified_sentiment_dictionary %>% 
  pivot_longer(-1) %>% 
  group_by(word) %>% 
  count(value) %>% 
  slice(which.max(n)) %>% 
  select(-n)

# %>% 
#     left_join(get_sentiments(lexicon = "bing") %>% 
#                 filter(str_detect(word, "envio"))
#               transmute(word, 
#                         original_sentiment = ifelse(sentiment == "positive", 1, -1)))


```


```{r}
metadata_df <- tibble::tribble(
  ~"Country", ~"Country code",            ~"Media outlet", ~"State-financed",     ~"Language",
  "Austria",          "AT",                    "Die Presse",            "No",    "German",
  "Belgium (French)",          "BE",                          "RTBF",           "Yes",  "French",
  "Belgium (Dutch)",          "BE",                           "VRT",           "Yes",  "Dutch",
  "Bulgaria",          "BG", "Bulgarian National Television",           "Yes",   "Bulgarian",
  "Cyprus",          "CY",                   "Cyprus Mail",            "No",    "English",
  "Czech Republic",          "CZ",                "Česká televize",           "Yes",     "Czech",
  "Denmark",          "DK",                     "Politiken",            "No",      "Danish",
  "United Kingdom",          "UK",                  "The Guardian",            "Nem",    "English",
  "Estonia",          "EE",                           "ERR",           "Yes",     "Estonian",
  "Finland",          "FI",                      "Yle News",           "Yes",     "Finnish",
  "France",          "FR",                     "France 24",           "Yes",  "French",
  "Greece",          "EL",                           "ERT",           "Yes",    "Greek",
  "Netherlands",          "NL",                           "NOS",            "No",  "Dutch",
  "Croatia",          "HR",                 "Večernji list",            "No",   "Croatian",
  "Ireland",          "IR",               "The Irish Times",            "No",    "English",
  "Iceland",          "IS",                           "RÚV",           "Yes",  "Icelandic",
  "Poland",          "PL",                           "TVP",           "Yes",  "Polish",
  "Latvia",          "LV",                           "LSM",           "Yes",     "Latvian",
  "Lithuania",          "LT",                "LRT televizija",           "Yes",   "Lithuanian",
  "Luxembourg",          "LU",                   "L'essentiel",            "No",  "French",
  "Hungary",          "HU",                     "hirado.hu",           "Yes",   "Hungarian",
  "Malta",          "MT",                           "TVM",           "Yes",   "Maltese",
  "Germany",          "DE",                   "DER SPIEGEL",            "No",    "German",
  "Norway",          "NO",                           "NRK",           "Yes",   "Norwegian",
  "Italy",          "IT",                 "la Repubblica",            "No",    "Italian",
  "Portugal",          "PT",                           "RTP",           "Yes", "Portuguese",
  "Romania",          "RO",                           "TVR",           "Yes",    "Romanian",
  "Spain",          "ES",                          "RTVE",           "Yes",  "Spanish",
  "Switzerland",          "CH",              "SWI swissinfo.ch",            "No",    "English",
  "Sweden",          "SE",                           "SVT",           "Yes",     "Swedish",
  "Slovakia",          "SK",                      "Nový Čas",            "No",  "Slovak",
  "Slovenia",          "SI",                 "RTV Slovenija",           "Yes",  "Slovene"
)

metadata_df <- dat %>% 
  group_by(country) %>% 
  mutate("Starting date" = as.character(ymd(min(date))), 
         "End date" = as.character(ymd(max(date)))) %>% 
  select(country, "Starting date", "End date") %>% 
  unique() %>% 
  merge(metadata_df, by = "country") %>% 
  rename("Country code" = country) %>% 
  arrange(Country)

select(metadata_df, 'Country', "Media outlet", "State-financed", "Starting date", "End date", "Number of articles") %>% 
  arrange("Country") %>% 
  knitr::kable(caption = "Description of the downloaded data from 31 country's media outlets.",
               align = c('l', rep('c', 5)))

```

## Google translate

```{r fig.cap="Leggyakrabban előforduló szavak a magyar nyelvű cikkekben a fordítást megelőzően és azt követően.", fig.height=8}
# Automatic translation =================================================================

load("data/raw/Germany_rawtext.RData")

Germany_rawtext %<>% 
  select(text) %>% 
  unnest_tokens(words, text)

st_de <- c(stopwords::stopwords('german'), "ab", "dass", "mehr", "worden", "wurde", "wurden", "sei") %>% 
  {ifelse(str_starts(., "új"), NA, .)} %>% 
  na.omit()

ggpubr::ggarrange(
  Germany_rawtext %>% 
    filter(!str_detect(words, '\\d')) %>% 
    anti_join(data.frame(words = st_de)) %>% 
    count(words, sort = T) %>% 
    arrange(desc(n)) %>% 
    head(29) %>% 
    mutate(
      words = fct_reorder(words, n)
    ) %>% 
    ggplot() +
    aes(n, words) + 
    geom_vline(xintercept = 0) +
    geom_col(color = 'black', fill = "gray70") +
    labs(title = 'Original text', x = 'Word count', y = NULL),
  dat_words_monthly %>% 
    filter(country == 'DE') %>% 
    group_by(country, words) %>% 
    summarise(n = sum(n)) %>% 
    ungroup() %>% 
    filter(!str_detect(words, '\\d')) %>% 
    anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
    arrange(desc(n)) %>%
    left_join(modified_bing, 
              by=c("words"="word")) %>% 
    head(29) %>% 
    mutate(
      value = case_when(
        value < 0 ~ "Negative",
        value > 0 ~ "Positive", 
        T ~ "No sentiment"
      ),
      words = fct_reorder(words, n)
    ) %>% 
    ggplot() +
    aes(n, words, fill = value) + 
    geom_vline(xintercept = 0) +
    geom_col(color = "black") +
    labs(title = 'After translation', x = 'Word count', y = NULL, 
         fill = "Word sentiment") +
    scale_fill_manual(values = c('red4', 'gray70', 'green')) + 
    theme(
      legend.position = 'bottom',
      legend.direction = 'horizontal'
    ), common.legend = T
)

```


```{r fig.cap = "A teljes korpusz leggyakoribb szavai, havonta", fig.height=10}
dat_words_monthly %>% 
  group_by(date, words) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  group_by(date) %>% 
  group_modify(~ mutate(.x, n = n / sum(n))) %>% 
  anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
  arrange(desc(n)) %>%
  group_modify(~ head(.x, 10)) %>%
  mutate(words = reorder_within(words, n, date)) %>%
  ggplot(aes(n, words)) + 
  geom_col(color = "black", fill = "cyan4") + 
  facet_wrap(~ date, scales = "free_y", labeller = as_labeller(function(x) str_sub(as.character(x), end = 7)))  +
  scale_y_reordered() + 
  labs(x = "Word frequency ratio", y = NULL)
```


## The pandemic

```{r timeline_plotting, fig.cap="Az új esetek száma és a szentimentet tartalmazó szavak száma naponta"}
library(ggforce)
library(glue)
library(ggtext)

dat_covid %>% 
  filter(code %in% unique(dat_sentiment_daily$country) &
           date < lubridate::ymd("2021-02-01") &
           date > lubridate::ymd("2019-12-31")
  ) %>% 
  group_by(date) %>% 
  summarise(new_cases = sum(new_cases, na.rm = T)) %>% 
  ungroup() %>% 
  merge(
    tibble(date = dmy(c("7th January 2020", "24th January 2020", "30th January 2020", 
                        "14th February 2020", "16th March 2020", "26th June 2020", 
                        "22nd September 2020", "23rd November 2020",
                        "21st December 2020")),
           text = c("The novel coronavirus is identified in Wuhan, China", 
                    "First confirmed European case in Bordeaux",
                    "The World Health Organization declares the outbreak a Public Health Emergency of International Concern",
                    "First confirmed European death in France",
                    "Many European countries deploy restrictions and declare state of emergency",
                    "Start of reopening in European nations",
                    "Owing to the easing during the summer, the number of new cases exceeds the first wave",
                    "Publishing of the third phase tests of vaccines Pfizer and AstraZeneca",
                    "The Pfizer vaccine is officially accepted by the European Medicines Agency")
    ), all = T
  ) %>% 
  tibble() %>% 
  merge(
    dat_sentiment_daily %>% 
      group_by(date) %>% 
      summarise(n_sentiment = sum(n)) %>% 
      ungroup(),
    all = T
  ) %>% 
  merge(
    tibble(date = seq.Date(from = lubridate::ymd('2019/12/31'), 
                           to = lubridate::ymd('2021/03/01'), by = "days")) %>% 
      mutate(t = row_number()), all.x = T
  ) %>% 
  tibble() %>% 
  mutate(
    new_cases = ifelse(is.na(new_cases), 0, new_cases),
    n_sentiment = ifelse(is.na(n_sentiment), 0, n_sentiment),
    new_cases = zoo::rollmean(new_cases, 7, na.pad=TRUE),
    n_sentiment = zoo::rollmean(n_sentiment, 7, na.pad=TRUE),
    n_sentiment = ifelse(is.na(n_sentiment), 0, n_sentiment),
    new_cases = ifelse(is.na(new_cases), 0, new_cases),
    date2 = ymd(ifelse(is.na(text), NA, as.character(date)))
  ) %>% 
  {
    
    ggplot(., aes(x =date, y = new_cases)) +
      geom_hline(yintercept = 0) +
      geom_line(color = "red") +
      geom_line(aes(date, n_sentiment*0.1), color = "blue") + 
      scale_y_continuous(
        name = "Number of daily new cases",
        sec.axis = sec_axis(~./0.1, name="Number of words with sentiment")
      ) + 
      geom_mark_circle(data = filter(., !is.na(text)), aes(x=date, y = n_sentiment, description = glue('"{text}"'),
                                                           label = glue("{date}:"), group = date), color = NA, 
                       expand = unit(2, "mm"), label.family = c("Oswald", "Poppins"), 
                       label.fontsize = 6,
                       label.buffer = unit(5, "mm"), con.size = 0.2) +
      labs(x = NULL, color = NULL) + 
      theme(
        legend.position = 'bottom'
      )
  }

#TODO Color error javítása
```

# Text analysis

## Topic model

```{r fig.cap='Finding the optimal number of topics for modelling', fig.height=2.5}
topic_model_df <- map(2:15, ~ {
  load(str_c("data/filtered_topic_model", ., ".RData"))
  get("mod")
}) %>% 
  enframe(value = "model", name = NULL) %>% 
  mutate(n_topics = 2:15)

topic_model_df %>% 
  mutate(
    l = map_dbl(model, logLik)
  ) %>% 
  ggplot(aes(n_topics, l)) + 
  geom_line() + 
  geom_point() + 
  scale_x_continuous(breaks = 2:15)
```

```{r fig.height=12, fig.cap='Leggyakoribb szavak topikonként'}

load("data/filtered_topic_model12.RData")

tidy(mod, matrix = "beta") %>%
  anti_join(rename(stop_words, term = word)) %>% 
  group_by(topic) %>%
  top_n(40, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term)) +
  geom_vline(xintercept = 0) +
  geom_col(show.legend = FALSE, color = 'black', fill = 'cyan4') +
  facet_wrap(~ topic, scales = "free_y", ncol = 3, labeller = as_labeller(
    function(x) paste('Topic', x)
  )) +
  scale_y_reordered() +
  labs(x = expression(beta), y = NULL)

#TODO labels as topic names


tidy(mod, matrix = "beta") %>% # unreported
  anti_join(rename(stop_words, term = word)) %>% 
  group_by(topic) %>%
  top_n(20, beta) %>%
  group_modify(.f = ~ tibble(terms = str_c(.x$term, collapse = ", "))) %>% 
  ungroup() %>% 
  knitr::kable(caption= "Most frequent words by topic", align = c("c", "l"))

```



```{r fig.cap="Topikok relatív megoszlásának időbeni dinamikája"}
dat_topics %>% 
  mutate(date = ym(str_sub(as.character(date), end = -4))) %>% 
  group_by(date, country) %>% 
  summarise_all(.funs = function(x)  mean(x, na.rm = T)) %>% 
  ungroup() %>% 
  select(date, starts_with('topic')) %>% 
  group_by(date) %>% 
  summarise_all(.funs = function(x)  mean(x, na.rm = T)) %>% 
  ungroup() %>% 
  pivot_longer(-1) %>% 
  mutate(name = factor(str_remove_all(name, 'topic_'), levels = as.character(1:12),
                       ordered = T)) %>% 
  ggplot() +
  aes(date, value, fill = name) +
  geom_col(color = 'black') + 
  labs(x = NULL, y = 'Relative frequency', fill = 'Topic') + 
  scale_y_continuous(labels = scales::percent) + 
  theme_minimal() +
  theme(
    legend.position = 'right',
    legend.direction = 'vertical'
  )

```

```{r fig.height=6, fig.cap='Topikok jellemzése szentiment, dátum és gyakoriság szerint'}
topic_descript_df <- tibble( 
  topic = c(
    "Restrictions", 
    "Schools",
    "General politics",
    "Hospitals",
    "Vaccine research",
    "Sport",
    "Global politics",
    "Vaccine politics",
    "Mental health",
    "General statistics",
    "Global statistics", 
    "Economy"
  ),
  n = dat_topics %>% 
    select(country, starts_with('topic')) %>% 
    group_by(country) %>% 
    summarise_all(.funs = function(x) mean(x, na.rm = T)) %>% 
    ungroup() %>% 
    select(-country) %>% 
    apply(2, mean),
  date = dat_topics %>% 
    select(country, date, starts_with('topic')) %>% 
    mutate(date = ym(str_sub(as.character(date), end = -4))) %>% 
    group_by(country, date) %>% 
    summarise_all(.funs = function(x) mean(x, na.rm = T)) %>% 
    ungroup() %>% 
    select(-country) %>% 
    group_by(date) %>% 
    summarise_all(.funs = function(x) mean(x, na.rm = T)) %>% 
    {
      apply(.[2:13], 2, function(x) {
        as.character(.$date)[which.max(x)]
      })
    } %>% 
    ymd(),
  sentiment = dat_sentiment %>% 
    select(country, sent_mean, sent_n, top_topic) %>% 
    na.omit() %>% 
    group_by(country, top_topic) %>% 
    summarise(sentiment = weighted.mean(x = sent_mean,
                                        w = sent_n,
    )) %>% 
    ungroup() %>% 
    group_by(top_topic) %>% 
    summarise(sentiment = mean(sentiment)) %>% 
    #arrange(desc(top_topic)) %>% 
    .$sentiment
) %>% 
  arrange(desc(n))

ggplot(data = topic_descript_df) +
  geom_hline(aes(yintercept = 0), linetype = 2, color = 'grey20') +
  geom_point(aes(x = date, y = (sentiment-mean(sentiment))*n, fill = sentiment, size = n)) +
  scale_size(range = c(10, 20), breaks = c(.1), 
             labels = function(x) scales::percent(x, accuracy = 1, decimal.mark = ',')) + # TODO
  scale_fill_gradient(low = 'cyan4', high = 'grey90', guide = guide_colorsteps()) +
  geom_text(mapping = aes(x = date, y = (sentiment-mean(sentiment))*n, label = topic), 
            show.legend = F, size = 3) + 
  geom_text(mapping = aes(x = ymd('2020-03-01'), y = -.00008), 
            color = 'grey20', size = 3,
            label = "Average of topics' sentiments") +
  labs(x = "Most typical month of topics", y = 'Sentiment influence',
       fill = 'Average sentiment', size = 'Relative frequency (size)') +
  theme_minimal() + 
  theme(
    legend.position = 'bottom',
    legend.box = "vertical",
    legend.key.width = unit(1, 'cm')
  )

```


## Sentiment

```{r}
modified_bing %>%
  left_join(get_sentiments(lexicon = "bing") %>% 
            transmute(word,
                      original_sentiment = ifelse(sentiment == "positive", 1, -1))) %>% 
  left_join(dat_words_monthly %>% 
              group_by(words) %>% 
              summarise(n = sum(n)),
            by = c("word" = "words")) %>% 
  filter(value != original_sentiment) %>% 
  arrange(desc(n)) %>% 
  mutate(value = ifelse(value == "NA", 0, value)) %>% 
  select(word, n, original_sentiment, value) %>% 
  head(23) %>% 
  kable(caption = "Most frequent words with changed sentiments because of corpus-specific context", col.names = c("Word", "Frequency", "Old sentiment", "New sentiment"))
```

```{r fig.cap="Leggyakrabban előforduló pozitív és negatív szentimenttel rendelkező szavak"}
library(reshape2)

dat_words_monthly %>% 
  group_by(country, words) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  filter(!str_detect(words, '\\d')) %>% 
  anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
  arrange(desc(n)) %>%
  left_join(modified_bing, 
            by=c("words"="word")) %>% 
  mutate(
    sentiment = ifelse(value > 0, "Positive", "Negative")
  ) %>% 
  na.omit() %>% 
  arrange(desc(n)) %>% 
  group_by(sentiment) %>% 
  group_modify(~ head(.x, 50)) %>% 
  ungroup() %>% 
  acast(words ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("cyan4", "red4"),
                   max.words = 100)

```


```{r fig.height=6, fig.cap='Szentimenttel bíró szavakkal korreláló szavak hálója'}
library(ggraph)
library(igraph)

set.seed(2021)

f_colorise <- function(x) {
  pos <- modified_bing %>% 
    filter(value == 1) %>% 
    pull(word)
  neg <- modified_bing %>% 
    filter(value == -1) %>% 
    pull(word)
  case_when(
    x %in% pos ~ 'green', #positive
    x %in% neg ~ 'red', #negative
    T ~ 'black' #neutral
  )
}

dat %>% 
  {.[sample(nrow(.)), ]} %>% 
  group_by(country) %>% 
  group_modify(~head(.x, 100)) %>% # TODO increase
  ungroup() %>% 
  mutate(r = row_number()) %>% 
  select(r, text) %>% 
  unnest_tokens(words, text) %>% 
  anti_join(data.frame(words = c(stopwords::stopwords(), "also", "can"))) %>% 
  count(r, words, sort = T) %>% 
  group_by(words) %>%
  filter(n() >= 20) %>%
  widyr::pairwise_cor(words, r, sort = TRUE) %>% 
  filter(item1 %in% modified_bing$word) %>% 
  head(100) %>% 
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(aes(color = f_colorise(name)), size = 5, shape = 16) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() + 
  labs(color = 'Sentiment')

#TODO color names

```

```{r}
dat_sentiment_daily %>% 
  group_by(date) %>% 
  summarise_at(c('n', 'sent_n'), .funs = function(x) sum(x, na.rm = T)) %>% 
  ggplot(aes(date, sent_n/n)) +
  geom_line(color = '#595959', size = .8) +
  geom_line(aes(date, zoo::rollmean(sent_n/n, 7, na.pad=TRUE), 
                color = 'Rolling 7-day average'), size = 1.3) +
  scale_color_manual(values = c('#E3120B')) + 
  labs(x = NULL, y = 'Ratio of words with sentiment', color = NULL)

```

```{r fig.cap="A szentiment alakulása országonként", fig.height=10, fig.width=15, out.extra='angle=90', fig.align ='center'}
# Explore the data ----------------------------------------------------------------------

dat_sentiment_daily %>% 
  mutate(code = country) %>% 
  ggplot(aes(date, sent_mean)) +
  geom_hline(yintercept = 0, color = "grey20") +
  geom_line(size = .3, color = 'grey50') +
  geom_smooth(size = 1.5, se = F) +
  facet_geo(~ code, grid = mygrid, label = 'name') +
  scale_x_date(limits = c(min(dat_sentiment_daily$date), max(dat_sentiment_daily$date)),
               breaks = c(min(dat_sentiment_daily$date), max(dat_sentiment_daily$date))) +
  labs(y = "Sentiment", x = NULL)

```

## TF-IDF

```{r tf_idf_per_month, fig.height=8, fig.cap="A cikkekben havonta leginkább jellemző szavak, TF-IDF értékek alapján"}
dat_words_monthly %>% 
  filter(words != "feff") %>% 
  mutate(month = ifelse(year(date)>2020,month(date)+12, month(date))) %>% 
  group_by(month, words) %>% 
  summarise(monthly_n = sum(n)) %>% 
  bind_tf_idf(words, month, monthly_n) %>%
  arrange(desc(tf_idf)) %>% 
  ungroup() %>% 
  filter(!(words %in% c(stopwords("english"), "also", "one", "will", "although"))) %>% 
  filter(!str_detect(words, "[0-9,]+")) %>% 
  filter(str_detect(words, "^[A-Za-z]+$")) %>% 
  filter(!str_detect(words, "serif")) %>% 
  filter(!(words %in% (dat_words_monthly %>% # filter words from ONE country
                         group_by(words) %>% 
                         mutate(nr_country = n_distinct(country))  %>% 
                         ungroup() %>% 
                         filter(nr_country < 3) %>% 
                         pull(words)))) %>% 
  filter(tf >0.00005) %>% 
  group_by(month) %>%
  slice_max(tf_idf, n = 3) %>%
  ungroup() %>%
  mutate(month = as.factor(month),
         words = reorder_within(words, tf_idf, month)
  ) %>% 
  ggplot(aes(tf_idf, y = words, fill = month)) +
  geom_vline(xintercept = 0) +
  geom_col(show.legend = F, color = 'black') +
  scale_y_reordered() +
  scale_fill_hue(h = c(200, 300)) +
  scale_x_continuous(breaks = c(0, 0.00005, 0.0001), limits = c(0, 0.0001), 
                     expand = c(0, 0)) +
  facet_wrap(~month, ncol = 3, scales = "free_y", labeller = as_labeller(function(x) {
    c(paste0('2020-', 1:12), '2021-01')[as.numeric(x)]
  })
  ) +
  labs(x = "TF-IDF", y = NULL)
```

# Econometrics

## Descriptive statistics

```{r}
dat_plm <- 
rename(dat_sentiment_monthly, code = country, date = m) %>% 
  left_join(dat_eco_sent %>% 
          filter(indic == "BS-ESI-I") %>% 
  select(date = time, code = geo, eco = values)) %>% 
  left_join(dat_unemployment) %>% 
  left_join(dat_covid_monthly) %>% 
  mutate(
    t = lubridate::interval(lubridate::ymd('2020-01-01'), date),
    t = lubridate::as.period(t) %/% months(1),
    cases = ifelse(is.na(cases), 0, cases),
    death = ifelse(is.na(death), 0, death),
    new_cases = ifelse(is.na(new_cases), 0, new_cases),
    new_deaths = ifelse(is.na(new_deaths), 0, new_deaths)
  ) %>% 
  select(-n) %>% 
  pivot_longer(-c(1:2)) %>% 
  {
    rbind(.,
          mutate(., 
                 name = paste0(name, '_l'),
                 date = date %m+% months(1)
          )
    )
  } %>%
  pivot_wider(names_from = name, values_from = value) %>% 
  mutate(
    cases_l = ifelse(is.na(cases_l), 0, cases_l),
    death_l = ifelse(is.na(death_l), 0, death_l)
  ) %>% 
  mutate(t_2 = t*t) %>% 
  mutate(
    season = case_when(
      date < ymd('2020-03-01') ~ 'Winter 2019/2020',
      date < ymd('2020-06-01') ~ 'Spring 2020',
      date < ymd('2020-09-01') ~ 'Summer 2020',
      date < ymd('2020-12-01') ~ 'Autumn 2020',
      T ~ 'Winter 2020/2021'
    ),
    season = factor(season, levels = c('Winter 2019/2020', 'Spring 2020', 'Summer 2020', 'Autumn 2020',
                                       'Winter 2020/2021'))
  ) %>% 
  filter(!is.na(sent_mean)) %>% 
  select(code, date, everything())

```

```{r fig.cap = "Regressziós fa"}
# Regression tree -----------------------------------------------------------------------
dat_plm %>% 
  select(sent_mean, eco, unemployment, cases, death, new_cases, new_deaths, t) %>% 
  set_names('Sentiment', 'ESI', 'Unemployment', 'Number of cases (per 1000)', 'Number of deaths (per 1000)',
            'Number of cases', 'Number of deaths', 't') %>% 
  rpart::rpart(formula = Sentiment ~.,
               cp = .012) %>% 
  rattle::fancyRpartPlot(palettes = 'PuRd', sub = NULL)

```

```{r fig.cap = "A hírekben megjelenő szentiment és gazdaság érzékelési index közötti kapcsolat"}
dat_plm %>% 
  mutate(
    date = str_sub(as.character(date), end = -4)
  ) %>% 
  ggplot(aes(eco, sent_mean, fill = season, label = paste(date, code))) + 
  geom_point(size = 2) +
  ggrepel::geom_text_repel(max.overlaps = 5) +
  theme_bw() + 
  theme(legend.position = 'bottom') +
  scale_fill_brewer(palette = "Dark2") +
  labs(x = 'Economic Sentiment Index', y = 'Media sentiment', fill = NULL)

```

## Panel models

```{r}
panel_models <- tibble(
  formula = c(
    "sent_mean ~ death + eco",
    "sent_mean ~ death + eco + season + death:t"
  )
) %>% 
  mutate(
    pooling = map(formula, ~ plm::plm(data = dat_plm, formula = eval(.), model = "pooling")),
    within = map(formula, ~ plm::plm(data = dat_plm, formula = eval(.), model = "within")),
    # random model cannot be estimated
    pooltest_pvalue = pmap(list(pooling, within), plm::pooltest),
    pooltest_pvalue = map_dbl(pooltest_pvalue, "p.value"),
    r_within = map_dbl(within, plm::r.squared, dfcor = T)
  )

knitr::kable(select(panel_models, -pooling, -within))
```

```{r results='asis'}
pwalk(list(model = pull(panel_models, within), 
           title = c("Regressziós eredmények időszakra való kontrollálás nélkül", 
                     "Becsült regresszió paraméterek kontrollálva az időszakokra")), 
      function(model, title) {
        tidy(model) %>% 
          kable(caption = title) %>% 
          print()
      })
```

